library(devtools)
library(bigmemory)
library(parallel)

install_github("wccarleton/chronup")
library(chronup)

# The data preparation occurred in several self-contained steps to avoid
# cumulative increases in computation during debugging---having to do each step
# over again to double check something or fix a bug is very time-consuming. So,
# each step below is nested in an if-statement. The statements are controlled
# by the following binary vector. The first element refers to the first step,
# the second element to the second step, and the third to the final step.
# Simply toggle the elements between 1 and 0 to perform or skip the relevant
# step, respectively.

# The datasets generated by this script are quite large. Some reflect
# intermediary processing steps. The final key ones---those needed to run the
# regression analyses described in the associated paper---have also been
# archived with Zenodo: 10.5281/zenodo.5354992 so they can be downloaded and
# used directly without the need to re-run this script.

steps <- c(1,  # Step 1
            1, # Step 2
            1, # Step 3
            1, # Step 4
            1) # Step 5

# Several of the steps involve parallel processing to speed things up. The
# following lines are preparing the cluster.

wd <- getwd()
ncores <- parallel::detectCores()
cl <- parallel::makeCluster(ncores - 1)
parallel::clusterEvalQ(cl,{
                    wd <- getwd()
                    })

# Create a set of equally-spaced time-stamps---defining the temporal grid onto
# which all data will be sampled. These center points are the mid-points of the
# temporal bins that will be used for the Negative-Binomial regression. At this
# point, they have to be in BCAD to conform to the OxCal output and then BP for
# everything else. The target interval is 3500--1750 BC and we will use 20-year
# time-bins.

breaks_BCAD <- seq(-3500,-1730,20)
midpoints_BCAD <- chronup::mids(breaks_BCAD)
breaks_BP <- 1950 - breaks_BCAD
midpoints_BP <- 1950 - midpoints_BCAD

# Define some paths to the data. These can be changed to explore different
# isotope records or subsets of the C14 database from Malta. The formats, of
# course, would have to be the same for the rest of the data preparation script
# to work as expected. Best to run this script in separate folders to keep the
# data generated by runs involving different paths (different datasets).


c14_dates_path <- "Data/malta_dates.csv"
isotope_data_path <- "Data/RL4-016_iso_measures.csv"
oxcal_age_model_path <- "Data/RL4_Sisal_modelled.csv"

# The OxCal mcmc samples need to be generated from Oxcal. The file is ~1.2 GB,
# which means it cannot be easily shared/archived via GitHub. The one we
# obtained from OxCal and used in our analysis will be available at
# URL
oxcal_mcmc_samples_path <- "Data/Big/oxcal_mcmc_samples_RL4_Sisal.csv"

# Step 1
# Get OxCal age ensemble---these are in BCAD! Then, interpolate to sample the
# model at all of the depths in the RL4 isotope dataset. This could not be done
# in OxCal while also producing a usable model (with a high overall agreement).
# It also really slowed OxCal down to try.

if(steps[1]){
    print("Running step 1...")

    oxcal_mcmc_samples <- as.matrix(read.csv(oxcal_mcmc_samples_path))

    mcmc_samples_dim <- dim(oxcal_mcmc_samples)
    last_two <- c(mcmc_samples_dim[2] - 1,
                mcmc_samples_dim[2])

    oxcal_age_ensemble <- as.big.matrix(
        t(oxcal_mcmc_samples[,-c(1,2,last_two)]),
        backingpath = "Data/Big/",
        backingfile = "oxcal_age_mat",
        descriptorfile = "oxcal_age_desc")
}

if(steps[2]){
    print("Running step 2...")
    iso_data <- read.csv(isotope_data_path)
    iso_data <- subset(iso_data,!is.na(d18O))
    iso_age_depth_model <- read.csv(oxcal_age_model_path)
    model_depths <- iso_age_depth_model$z
    iso_depths <- iso_data$Sampled.Distance

    oxcal_age_ensemble <- bigmemory::attach.big.matrix("Data/Big/oxcal_age_desc")
    nsamples <- dim(oxcal_age_ensemble)[2]

    isotope_age_ensemble <- bigmemory::filebacked.big.matrix(
                                nrow = length(iso_depths),
                                ncol = nsamples,
                                backingpath = "Data/Big",
                                backingfile = "iso_age_mat",
                                descriptorfile = "iso_age_desc")

    # Wrapper for parallel bigmatrix (linear) interpolation

    interpolate_ages <- function(j,
                            model_depths,
                            sample_depths,
                            bigmatrix_model,
                            bigmatrix_out){
        a <- bigmemory::attach.big.matrix(bigmatrix_model)
        m <- bigmemory::attach.big.matrix(bigmatrix_out)
        sampled_ages <- approx(x = model_depths,
                                y = a[, j],
                                xout = sample_depths)$y
        m[, j] <- sampled_ages
        return()
    }

    pbapply::pbsapply(
                cl = cl,
                X = 1:nsamples,
                FUN = interpolate_ages,
                model_depths = model_depths,
                sample_depths = iso_depths,
                bigmatrix_model = paste(wd,"/Data/Big/oxcal_age_desc",sep=""),
                bigmatrix_out = paste(wd,"/Data/Big/iso_age_desc",sep=""))
}

# Step 3
# Create an ensemble of probable isotope series given the uncertainty in the
# ages from the age-depth model

if(steps[3]){
    print("Running step 3...")
    # Get isotope measurement data

    iso_data <- read.csv(isotope_data_path)
    iso_data <- subset(iso_data,!is.na(d18O))

    # In case Step 1 wasn't run, we need to re-attach the iso_age_mat to
    # determine how many isotope record samples (different probable series
    # based on different probable age-depth relationships) are required.

    age_ensemble <- bigmemory::attach.big.matrix("Data/Big/iso_age_desc")
    nsamples <- dim(age_ensemble)[2]

    # make a bigmemory matrix to store the sampled isotope series

    isotope_ensemble <- bigmemory::filebacked.big.matrix(
                            nrow = length(midpoints_BCAD),
                            ncol = nsamples,
                            backingpath = "Data/Big",
                            backingfile = "iso_mat",
                            descriptorfile = "iso_desc")

    # Simple wrapper for parallel bigmatrix isotope smoothing/sampling with
    # ksmooth. The returned isotope samples will be ordered in forward-time
    # (oldest to youngest) from top to bottom.

    sample_isotopes <- function(j,
                            y,
                            time_grid_centers,
                            bigmatrix_age,
                            bigmatrix_out){
        a <- bigmemory::attach.big.matrix(bigmatrix_age)
        m <- bigmemory::attach.big.matrix(bigmatrix_out)

        d <- data.frame(y = y, x = a[, j])
        d <- subset(d, !is.na(y))

        sampled_smooth <- ksmooth(y = d$y,
                            x = d$x,
                            kernel = "normal",
                            b = 60,
                            x.points = time_grid_centers)
        sampled_y <- sampled_smooth$y
        m[, j] <- sampled_y
        return()
    }

    # use parallel to create the samples and fill in the matrix

    pbapply::pbsapply(
            cl = cl,
            X = 1:nsamples,
            FUN = sample_isotopes,
            y = iso_data$d18O,
            time_grid_centers = midpoints_BCAD,
            bigmatrix_age = paste(wd,"/Data/Big/iso_age_desc",sep=""),
            bigmatrix_out = paste(wd,"/Data/Big/iso_desc",sep=""))

    # We need to interleave the isotope ensemble with columns representing an
    # intercept for the new REC MCMC.

    iso_interleaved <- bigmemory::filebacked.big.matrix(
                            nrow = dim(isotope_ensemble)[1],
                            ncol = dim(isotope_ensemble)[2] * 2,
                            backingpath = "Data/Big",
                            backingfile = "iso_intercept_mat",
                            descriptorfile = "iso_intercept_desc")

    interleave_big_matrix <- function(j,
                                    v,
                                    bigmatrix_in,
                                    bigmatrix_out){
        m_in <- bigmemory::attach.big.matrix(bigmatrix_in)
        m_out <- bigmemory::attach.big.matrix(bigmatrix_out)
        index <- j + (j - 1)
        m_out[, index] <- v
        m_out[, index + 1] <- m_in[, j]
        return()
    }
    pbapply::pbsapply(
                cl = cl,
                X = 1:nsamples,
                FUN = interleave_big_matrix,
                v = 1,
                bigmatrix_in = paste(wd,"/Data/Big/iso_desc",sep=""),
                bigmatrix_out = paste(wd,"/Data/Big/iso_intercept_desc",sep=""))
}

# Step 4
# Build a rece of c14 samples. These are all in BP!

if(steps[4]){
    print("Running step 4...")
    isotope_ensemble <- bigmemory::attach.big.matrix("Data/Big/iso_desc")
    malta_dates <- read.csv(c14_dates_path)
    malta_caldates <- build_c14_matrix(malta_dates[,c(2,3)])

    nsamples <- dim(isotope_ensemble)[2]

    ce_matrix <- malta_caldates$ce_matrix

    # We will need temporal bins that correspond to the midpoints_BP created
    # above. But, they have to cover the span of time covered by the Malta C14
    # samples---result of a dependency on graphics::hist for binning the counts.

    mt1 <- malta_caldates$time_range[1]
    mt2 <- malta_caldates$time_range[2]

    times_Malta_span <- mt1:mt2

    t1 <- mt1 + (20 - (mt1 %% 20)) - 10

    t2 <- mt2 - (mt2 %% 20) - 10

    breaks_rece_BP <- seq(t1, t2, -20)

    malta_rece <- bigmemory::filebacked.big.matrix(
                        nrow = length(breaks_rece_BP) - 1,
                        ncol = nsamples,
                        backingpath = "Data/Big",
                        backingfile = "malta_rece_mat",
                        descriptorfile = "malta_rece_desc")

    pbapply::pbsapply(
            cl = cl,
            X = 1:dim(isotope_ensemble)[2],
            FUN = chronup::sample_event_counts,
            ce_matrix = ce_matrix,
            times = times_Malta_span,
            breaks = breaks_rece_BP,
            bigmatrix = paste(wd,"/Data/Big/malta_rece_desc",sep=""))

    # We then trim the RECE so that it spans only the target analytical
    # interval and can be matched up to the isotope record.

    rece_mids <- mids(breaks_rece_BP)
    rece_index <- which(rece_mids %in% midpoints_BP)

    rece_trimmed <- bigmemory::filebacked.big.matrix(
                        nrow = length(midpoints_BP),
                        ncol = nsamples,
                        backingpath = "Data/Big",
                        backingfile = "malta_rece_trim_mat",
                        descriptorfile = "malta_rece_trim_desc")

    trim_big_matrix <- function(j,
                            keep_index,
                            bigmatrix_in,
                            bigmatrix_out){
        m_in <- bigmemory::attach.big.matrix(bigmatrix_in)
        m_out <- bigmemory::attach.big.matrix(bigmatrix_out)
        m_out[, j] <- m_in[keep_index, j]
        return()
    }
    pbapply::pbsapply(
            cl = cl,
            X = 1:dim(malta_rece)[2],
            FUN = trim_big_matrix,
            keep_index = rece_index,
            bigmatrix_in = paste(wd,"/Data/Big/malta_rece_desc",sep=""),
            bigmatrix_out = paste(wd,"/Data/Big/malta_rece_trim_desc",sep=""))
}

# Step 5
# Supplementary data for an analysis involving the mean published RL4 age-depth
# model instead of one made with OxCal.

if(steps[5]){
    # For convenience (so the MCMC regression code doesn't need to be altered),
    # we need to create a big matrix containing repeat columns of the isotope
    # record reflecting the published RL4 age-depth model. We will not be
    # accounting for chronological uncertainty in this case (with respect to
    # the isotope data) because we do not have a way of producing the needed
    # age-depth ensembles from the published model.

    # get sample size
    age_ensemble <- bigmemory::attach.big.matrix("Data/Big/iso_age_desc")
    nsamples <- dim(age_ensemble)[2]

    isotope_data_path <- "Data/RL4_iso_measures.csv"
    iso_data <- read.csv(isotope_data_path)

    isotope_ensemble <- bigmemory::filebacked.big.matrix(
                            nrow = length(midpoints_BP),
                            ncol = nsamples * 2,
                            backingpath = "Data/Big",
                            backingfile = RL4_iso_mat",
                            descriptorfile = "RL4_iso_desc")

    sample_isotopes <- function(j,
                            y,
                            a,
                            time_grid_centers,
                            bigmatrix_out){
        m <- bigmemory::attach.big.matrix(bigmatrix_out)

        d <- data.frame(y = y, x = a)
        d <- subset(d, !is.na(y))

        sampled_smooth <- ksmooth(y = d$y,
                            x = d$x,
                            kernel = "normal",
                            b = 50,
                            x.points = time_grid_centers)
        sampled_y <- sampled_smooth$y
        if(any(is.na(sampled_y))){
            print(j)
            print(sampled_smooth)
        }

        # The ksmooth function always returns the samples sorted by x
        # ascending rather than the order of the x vector passed to the
        # function. Here this means that the non-negative BP dates in
        # midpoints_BP will be reversed relative to the direction of increasing
        # time. We have to flip it.

        index <- j + (j - 1)
        m[, index] <- 1
        m[, index + 1] <- rev(sampled_y)

        return()
    }

    # use parallel to create the samples and fill in the matrix

    pbapply::pbsapply(
            cl = cl,
            X = 1:nsamples,
            FUN = sample_isotopes,
            y = iso_data$d18O,
            a = iso_data$Age.MARCH.2018 * 1000,
            time_grid_centers = midpoints_BP,
            bigmatrix_out = paste(wd,"/Data/Big/RL4_iso_desc",sep=""))
}

# Clean up the cluster
parallel::stopCluster(cl)
rm(cl)
